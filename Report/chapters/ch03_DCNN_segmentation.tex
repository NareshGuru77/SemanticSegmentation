%!TEX root = ../report.tex

\chapter{Convolutional Neural Networks and Semantic Segmentation}

	In this chapter, we look into the basic concepts of neural networks and later, how such concepts have been used for the task of semantic segmentation. In section \ref{section:ann} we present an overview of Artificial Neural Networks. In \ref{section:cnn} we look into basic concepts underlying Convolutional Neural Networks (CNNs). In \ref{section:cnnseg} we look into how CNNs are adapted for the task of semantic segmentation.

\section{Artificial Neural Networks}
\label{section:ann}

Artificial Neural Networks (ANN), inspired by the neural networks in our brain, was designed to learn tasks without explicitly programming descriptive features of the concerned tasks. An ANN is made up of processing units called neurons which performs a non-linear transformation, using an activation function, of the weighted linear combination of inputs. Figure \ref{Fig:haykin_ann_n} illustrates a non-linear model of a neuron. Many such neurons are connected to one another in an ANN resulting in its ability to learn highly non-linear function mappings from input to output space. Figure \ref{Fig:haykin_ann_ml} illustrates a multilayer feedforward neural network which is a type of ANN.

	\begin{figure}
		\centering
		\begin{subfigure}{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{images/neuron}
			\caption{}
			\label{Fig:haykin_ann_n}
		\end{subfigure}
		\begin{subfigure}{.3\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{images/ml_ff_nn}
			\caption{}
			\label{Fig:haykin_ann_ml}
		\end{subfigure}
		\caption{(a) A non-linear neuron model. $v_k$ = $\sum_{i=1}^{m} w_{ki} x_i$ denotes the local field of the neuron, $\varphi$ is the activation function and $y = \varphi(v_k)$ is the output of the neuron. (b) An illustration of a multilayer feedforward neural network with input source nodes, one hidden layer of neurons and an output layer of neurons \cite{haykin}.}
		\label{Fig:haykin_ann}
	\end{figure}

\section{Convolutional Neural Networks}
\label{section:cnn}

Convolutional Neural Networks (CNNs or Conv Nets) are a class of feedforward neural networks which are used in the field of computer vision. Tasks such as image classification, object detection and semantic segmentation make use of CNNs. Unlike the multilayer feedforward network shown in Figure \ref{Fig:haykin_ann_ml}, CNNs are designed to handle image data which is usually represented as a stack of 2 dimensional values. A major part of CNNs comprise the use of convolutional layers which greatly reduce the number of parameters in comparison to fully-connected layers.

\subsection{CNN Architecture}

The general architecture of a CNN is illustrated in Figure \ref{Fig:cnn_arch}. A CNN is composed of different layer types such as convolutional layer, pooling layer, normalization layer and pooling layer. Each of the layers are looked into in detail in the subsequent sections.
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=1\linewidth]{images/cnn_matlab}
		\caption{This figure illustrates a CNN architecture. A convolution layer convolves on the input image or feature maps followed by an ReLU activation which produces output feature maps. A pooling layer downsamples feature maps. The CNN architecture performs feature learning and classification \cite{matlab_cnn}.}
		\label{Fig:cnn_arch}
	\end{figure}

\subsubsection{Convolutional layer}

An RGB image typically consists of height, width and 3 channels representing red, blue and green. In order to better handle this structure of an input image, the neurons in a convolutional layer are arranged in a 3D volume of height, width and depth. Each neuron is only connected to a small region in the input. This is called "local connectivity" and the extent of the input region to which a neuron connects is controlled by a hyperparameter called receptive field \cite{cs231n}. An illustration of the 3D arrangement of neurons can be seen in Figure \ref{Fig:cnn_neuron}. When moving along the depth dimension, each 2D arrangement of values are called filters or kernels.

	\begin{figure}[h]
		\centering
		\includegraphics[width=.4\linewidth]{images/conv_3dneurons}
		\caption{Illustration of 3D (height, width, depth) arrangement of neurons in a convolutional layer. The convolutional layer transforms a 3D input volume to a 3D output volume  \cite{cs231n}.}
		\label{Fig:cnn_neuron}
	\end{figure}
	
A filter slides through an input channel, performs convolution and produces output feature maps. A filter looks for specific features like edges and corners, at the input spatial location it convolves over. As an extention, a filter can be seen as a template image and it looks for the template across the input space. Figure \ref{Fig:convolution} illustrates one such filter which looks for vertical lines. 

	\begin{figure}[h]
		\centering
		\includegraphics[width=.8\linewidth]{images/convolution}
		\caption{A Sobel kernel convolving over the input. This kernel looks for vertical edges in the input image. The kernel values are multiplied elementwise with the input locations and the results are added together to get the value at the location in the output feature map which corresponds to the center of the image locations  \cite{freecodecamp}.}
		\label{Fig:convolution}
	\end{figure}
	
The entire 3D convolution filter slides along the height and width of all channels of the input space looking for features along spatial locations and features across input channels. One 3D convolution filter results in a 2D feature map. Multiple such 3D convolutions are used each of which leads to a 2D feature maps which are all stacked along the depth dimension to create the output 3D volume. This process is illustrated in Figure \ref{Fig:3dconv}.

	\begin{figure}[h]
		\centering
		\includegraphics[width=.3\linewidth]{images/3d_conv}
		\caption{Illustration of 2D feature maps produced by two 3D convolution filters which are arranged along the depth dimension  \cite{towardsdatascience}.}
		\label{Fig:3dconv}
	\end{figure}
	
In CNNs, the filters are learnt through training the CNN for a specific task such as image classification. Once a CNN is trained, the filters in the lower layers often tend to learn simple features such as edges and corners and filters in the higher layers learn more abstract features building upon the features learned by filters in the previous layer. 

	\begin{figure}[h]
		\centering
		\includegraphics[width=.8\linewidth]{images/conv_template}
		\caption{Learned filters of AlexNet CNN architecture. The filters have learned to detect features such as lines, blobs and so on \cite{cs231n}.}
		\label{Fig:conv_template}
	\end{figure}
	

\subsubsection{Pooling layer}

A pooling layer is reponsible for reducing the spatial size of the feature maps leading to reduction in the number of trainable parameters in the network. This reduction in trainable parameters leads to reduced chances of overfitting and reduced computational cost. Additionally, pooling makes CNNs invariant to translations in the input image. This induced translation invariance is desired for tasks such as image classification where it is only required to find objects in an input space irrespective of where the object is located within the image. 

A pooling filter looks at a particular window in a depth slice (one 2D feature map out of 3D input feature maps) of input feature maps and produces one value as output for the window. Two common pooling types are max pooling and average pooling. The pooling filter outputs the maximum of all values and average of all values for max pooling and average pooling respectively. An illustration of max pooling and reduction in spatial size of feature maps is shown in Figure \ref{Fig:pool}.

	\begin{figure}[h]
	\centering
		\begin{subfigure}{.35\textwidth}
  			\centering
  			\includegraphics[width=1\linewidth]{images/pooling}
  			\caption{}
  			\label{Fig:poolb}
		\end{subfigure}
		\begin{subfigure}{.55\textwidth}
  			\centering
  			\includegraphics[width=1\linewidth]{images/maxpool}
  			\caption{}
  			\label{Fig:poola}
		\end{subfigure}
		\caption{(a) Illustration of feature maps being downsampled by pooling \cite{towardsdatascience}. (b) Illustration of max pooling where the pooling filter looks at 2$\times$2 input windows. Stride 2 denotes that the window moves two steps when it moves through x or y. Each color in the depth slice of input feature maps indicates a different input window and the corresponding color in the output feature map shows the selected max value \cite{cs231n}.}
		\label{Fig:pool}
	\end{figure}	


\subsubsection{Fully-connected layer}



\section{CNNs for Semantic Segmentation}
\label{section:cnnseg}
