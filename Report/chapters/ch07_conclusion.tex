%!TEX root = ../report.tex

\chapter{Conclusions}

In this work, state-of-the-art deep learning methods for semantic segmentation was reviewed. One particular model called DeepLabv3+ with MobileNetv2 and Xception network backbones was selected. A dataset consisting of 18 atWork objects was created. This dataset also consists of artificial images generated using a artificial image generation algorithm to augment the dataset. Two different datasets called variety of backgrounds dataset and white backgrounds dataset was created which differ in terms of the background images used for the artificial image generation. Four different variants for each of the two datasets were created based on the properties of the objects in the dataset. The DeepLabv3+ model was trained on the created dataset and its performance was assessed in terms of mIOU. A quantized version of the DeepLabv3+ model was then created and was compared with the full precision DeepLabv3+ models.

\section{Contributions}

The major contributions of this work includes:
	\begin{itemize}
		\item The creation of a semantic segmentation dataset consisting of 18 atWork objects.
		\item Augmenting this dataset with artificial images created by coding an artificial image generation algorithm.
		\item Training DeepLabv3+ models with resource efficient network backbones and analyzing its performance.
	\end{itemize}

\section{Lessons learned}

Lessons learned out of this work are listed below:
	\begin{itemize}
		\item Creating a dataset for a machine learning task is often time consuming. Capturing all the images at one stretch often introduces a bias in the dataset as most images end of being taken under the same real world conditions. A best practice would be to progressively create a dataset over a period of many days. During this time, insights from a trained model could be gathered to further improve the variety of the next round of images taken.
		\item Useful insights could be obtained by looking into the available data in different perspectives such as how two objects are different and what would happen if they are treated the same. Semantic segmentation models with the same hyperparameter settings could be trained on these different perspectives of data which could help understand how the model behaves in response to data.
	\end{itemize}

\section{Future work}

The dataset created using the atWork objects at present consist of real images which only have one object per image. Further real images which have multiple objects in them could be added to the dataset. This would enable better assessement of the effectiveness of the artificial images in terms of aiding generalization. The trained DeepLabv3+ models currently available as a result of this work could in theory reduce the labeling cost of the additional real images. Also, additional augmentation methods such as random brightness, contrast shifts, random rotations, and others could be incorporated with the artificial image generation algorithm to improve the robustness of the generated artificial images. In addition, the use 3D CAD models of the objects in the dataset available in \cite{atwork_models} and \cite{atwork_models_rockin} to further improve the robustness of the generated artificial images could be considered.

The dataset created is seemingly biased towards larger objects. This bias becomes worse in the atWork\_size\_invariant and atWork\_similar\_shapes variants in which some large objects are combined. This bias of the dataset affects the learning process and evidently, the learned DeepLabv3+ models were shown to ignore small objects. An attempt made to modify the loss function to induce class balancing has failed. This failure could be further analyzed. As an alternative approach, reducing the number of real images of large objects in comparison to smaller objects could be considered. For instance, only 2 images of containers and 22 images of "distance\_tube" could be used for the training set instead of the 22 images of each.

The experiments were centered on the created dataset and did not consider hyperparameter tuning to improve results. Further tuning the different hyperparameters involved such as the decoder output stride, regularization parameter (L2 weight decay) and so on could be explored. 

The initial goal of this work also included the use of pruning technqiues to compress the trained models. Pruning was not sufficiently explored in order to be implemented. The compressed DeepLabv3+ model with the MobileNetv2 network backbone using quatization suffered a high loss in mIOU of around 9 \%. This is due to the naiveness involved in directly quantizing for the sake of compression. A better approach would be to first prune redundant weights, filters or feature maps and later perform quantization which is reported to be effective in the literature. Recently, reinforcement learning has been applied to train an agent to prune a CNN model \cite{DBLP:journals/corr/abs-1801-07365}. This approach could also be explored.

